{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "Oddsportal has dynamic content, i.e. javascript rendered page. I used `selenium` package to load and render url content with Morzilla Firefox then parsed source code to `BeautifulSoup` to extract information of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import bs4\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from psw import psw, usr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_href(soup, league, season):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        soup: bs4.BeautifulSoup element (HTML source code parced with selenium webdriver)\n",
    "        league: string, e.g. NBA or Euro\n",
    "        season: string  e.g. 2017/1018 or 2013/2014\n",
    "    Output:\n",
    "        List of list. Each list element contains a size of 3\n",
    "        [league, season, url (unique for each game)]\n",
    "    \"\"\"\n",
    "    _data = list()\n",
    "    rows = soup.tbody.findAll('tr')\n",
    "    for row in rows:\n",
    "        if len(row.contents) == 7:\n",
    "            href = row.contents[1].find('a', href=True)\n",
    "            if href is not None:\n",
    "                href = \"https://www.oddsportal.com\" + href['href']\n",
    "                _data.append([league, season, href])\n",
    "    return _data\n",
    "\n",
    "def convert_2_int(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        string: e.g. \"123\" or \"93\"\n",
    "    Output:\n",
    "        int:\n",
    "    If ValueError, print problematic string and return NAN value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(string)\n",
    "    except ValueError:\n",
    "        print(\"Input {} could not be converted to integer\".format(string))\n",
    "        return np.nan\n",
    "    \n",
    "def convert_2_float(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        string: e.g. \"123\" or \"93\"\n",
    "    Output:\n",
    "        int:\n",
    "    If ValueError, print problematic string and return NAN value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(string)\n",
    "    except ValueError:\n",
    "        print(\"Input {} could not be converted to float\".format(string))\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate webdriver\n",
    "`executable_path` points to excecutable used to connect to Firefox. To use different browser download approprate geckodriver (hyperlink) or refer to this Stack exchange post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "driver = webdriver.Firefox(executable_path=r\"geckodriver.exe\")\n",
    "\n",
    "# go to Oddsportal website\n",
    "driver.get(\"https://www.oddsportal.com\")\n",
    "\n",
    "# click on log-in button\n",
    "driver.find_element_by_tag_name('button').click()\n",
    "\n",
    "# enter User name and psw\n",
    "driver.find_element_by_id('login-username1').send_keys(usr)\n",
    "driver.find_element_by_id('login-password1').send_keys(psw, Keys.ENTER)\n",
    "\n",
    "# set timeout for page loadding to 30 sec\n",
    "driver.set_page_load_timeout(30)\n",
    "\n",
    "# set wait element for explicit wait\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spesific football league season data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "league = \"EPL\"\n",
    "season = \"2015_2016\"\n",
    "season_url = \"https://www.oddsportal.com/soccer/england/premier-league-2015-2016/results/#/page/\"\n",
    "# number of pages for the season\n",
    "no_pages = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# temporary list to store data\n",
    "_data = list()\n",
    "\n",
    "for idx in range(1, no_pages+1):\n",
    "    # Load page\n",
    "    driver.get(season_url+str(idx))\n",
    "    # explicitly wait untill page is loaded\n",
    "    wait.until(EC.visibility_of_element_located((By.ID, 'tournamentTable')))\n",
    "    # Process HTLM into data\n",
    "    soup = bs4.BeautifulSoup(driver.page_source)\n",
    "    _data += get_unique_href(soup, league, season)\n",
    "\n",
    "# create dataframe with unqiue URLS\n",
    "df_urls = pd.DataFrame(_data, columns=[\"League\", \"Season\", \"URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for analysing match info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_names(soup):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        soup: BS4 soup element\n",
    "    \"\"\"\n",
    "    try:\n",
    "        names = soup.h1.text.split(\" - \")\n",
    "        return names[0], names[1]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Team names not found\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "            \n",
    "def load_page(driver, url):\n",
    "    \"\"\"\n",
    "    Return true if page was loaded correctly, else if error occured\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"page not loaded\")\n",
    "        print(f\"{e}\")\n",
    "        return False\n",
    "    \n",
    "def get_match_date(soup):\n",
    "    \"\"\"\n",
    "    Return date as string\n",
    "    \"\"\"   \n",
    "    try:\n",
    "        match_date = soup.find(\"div\", {\"id\": \"col-left\"}).p.text\n",
    "        datetime_object = datetime.strptime(match_date, '%A, %d %b %Y, %H:%M')\n",
    "        return datetime_object\n",
    "    except:\n",
    "        return np.nan  \n",
    "       \n",
    "def get_score(soup):\n",
    "    \"\"\"\n",
    "    Return\n",
    "    \"\"\"\n",
    "    element = soup.find(\"p\", {\"class\": \"result\"})\n",
    "    # calculate full-time score\n",
    "    score_FT = element.contents[1].text.split(\":\")\n",
    "    score_home_FT = convert_2_int(score_FT[0])\n",
    "    score_away_FT = convert_2_int(score_FT[1])\n",
    "    # calculate half-time scores\n",
    "    score_HT = element.contents[2].split(\",\")[0].split(\":\")\n",
    "    score_home_HT = convert_2_int(score_HT[0][2:])\n",
    "    score_away_HT = convert_2_int(score_HT[1])\n",
    "\n",
    "    return [score_home_FT, score_away_FT, score_home_HT, score_away_HT]\n",
    "        \n",
    "def norm_prop(list_of_odds):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        list_of_odds contains a list of odds\n",
    "    Output:\n",
    "        list of implied probabilities\n",
    "    \"\"\"\n",
    "    list_of_odds = 1/np.array(list_of_odds)\n",
    "    vigor = np.sum(list_of_odds)\n",
    "    return list_of_odds/vigor\n",
    "\n",
    "def calculate_stats(stats):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        stats: list, list of probabilities\n",
    "    Output:\n",
    "        list, [avg, median, std]\n",
    "    \"\"\"\n",
    "    stats = np.array(stats)\n",
    "    _max = stats.max().round(3)\n",
    "    _avg = stats.mean().round(3)\n",
    "    _median = np.median(stats).round(3)\n",
    "    _std = stats.std().round(3)\n",
    "\n",
    "    return [_max, _avg, _median, _std]\n",
    "\n",
    "def process_1x2_data(soup):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        soup: bs4 element\n",
    "        single: bool, if Ture then returns info about single event 1x2 odds\n",
    "    Output:\n",
    "        list of lists\n",
    "    \"\"\"\n",
    "    # create empty list to store odds and probabilities for spesific bookmakers\n",
    "    pinn_odds = [np.nan]*3\n",
    "    pinn_prop = [np.nan]*3\n",
    "    \n",
    "    # create empty lists to store home, draw and away probabilities\n",
    "    home_stats = list()\n",
    "    draw_stats = list()\n",
    "    away_stats = list()\n",
    "    \n",
    "    home_odds = list()\n",
    "    draw_odds = list()\n",
    "    away_odds = list()\n",
    "\n",
    "    _data = list()\n",
    "\n",
    "    rows = soup.tbody.findAll(\"tr\")\n",
    "    for row in rows:\n",
    "        content = row.findAll(\"td\")\n",
    "\n",
    "        if len(content) == 6:\n",
    "            # read odds for each bookmaker\n",
    "            book_name = content[0].text.replace(u'\\xa0', u'')\n",
    "            home_o = convert_2_float(content[1].text)\n",
    "            draw_o = convert_2_float(content[2].text)\n",
    "            away_o = convert_2_float(content[3].text)\n",
    "            # save odds to lists\n",
    "            home_odds.append(home_o)\n",
    "            draw_odds.append(draw_o)\n",
    "            away_odds.append(away_o)\n",
    "\n",
    "            # calculate probabilities\n",
    "            _odds = [home_o, draw_o, away_o]\n",
    "            _props = (1/norm_prop(_odds)).round(2)\n",
    "            home_p = _props[0]\n",
    "            draw_p = _props[1]\n",
    "            away_p = _props[2]\n",
    "            # add probabilities to list for future calculations\n",
    "            home_stats.append(home_p)\n",
    "            draw_stats.append(draw_p)\n",
    "            away_stats.append(away_p)\n",
    "\n",
    "            # read spesific bookmaker data\n",
    "            if book_name == \"Pinnacle\":\n",
    "                pinn_prop = _props.tolist()\n",
    "                pinn_odds = _odds\n",
    "    \n",
    "    # calculate stats\n",
    "    home_stats = calculate_stats(home_stats)\n",
    "    draw_stats = calculate_stats(draw_stats)\n",
    "    away_stats = calculate_stats(away_stats)\n",
    "    \n",
    "    \n",
    "    home_odds = np.array(home_odds)\n",
    "    draw_odds = np.array(draw_odds)\n",
    "    away_odds = np.array(away_odds)\n",
    "    \n",
    "    home_max = home_odds.max()\n",
    "    draw_max = draw_odds.max()\n",
    "    away_max = away_odds.max()\n",
    "\n",
    "    _data.append([\"home\", pinn_prop[0], pinn_odds[0], home_max] + home_stats)\n",
    "    _data.append([\"draw\", pinn_prop[1], pinn_odds[1], draw_max] + draw_stats)\n",
    "    _data.append([\"away\", pinn_prop[2], pinn_odds[2], away_max] + away_stats)\n",
    "    return _data    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_driver(driver):\n",
    "    \"\"\"\n",
    "    Turns off and on webdriver\n",
    "    \"\"\"\n",
    "    # turn off driver\n",
    "    driver.quit()\n",
    "\n",
    "    # initiate again driver\n",
    "    driver = webdriver.Firefox(executable_path=r\"geckodriver.exe\")\n",
    "\n",
    "    # go to Oddsportal website\n",
    "    driver.get(\"https://www.oddsportal.com\")\n",
    "\n",
    "    # click on log-in button\n",
    "    driver.find_element_by_tag_name('button').click()\n",
    "\n",
    "    # enter User name and psw\n",
    "    driver.find_element_by_id('login-username1').send_keys(usr)\n",
    "    driver.find_element_by_id('login-password1').send_keys(psw, Keys.ENTER)\n",
    "\n",
    "    # set timeout for page loadding to 30 sec\n",
    "    driver.set_page_load_timeout(30)\n",
    "\n",
    "    # set wait element for explicit wait\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    return driver, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver, wait = refresh_driver(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# temporary list to store data\n",
    "_data = list()\n",
    "failed_urls = list()\n",
    "\n",
    "i = 1\n",
    "\n",
    "for url in df_urls.URL.values[:30]:\n",
    "    if load_page(driver, url):\n",
    "        # wait till page is loaded\n",
    "        wait.until(EC.visibility_of_element_located((By.ID , \"odds-data-table\")))\n",
    "        # get soup element\n",
    "        soup = bs4.BeautifulSoup(driver.page_source)\n",
    "        # read team names\n",
    "        home_n, away_n = get_team_names(soup)\n",
    "        # get match date\n",
    "        match_date = get_match_date(soup)\n",
    "        # get score\n",
    "        score = get_score(soup)\n",
    "        \n",
    "        # get 1x2 stats\n",
    "        _ = process_1x2_data(soup)\n",
    "        \n",
    "        # add all data\n",
    "        _data.append([league, season, match_date, home_n, away_n, url] + score + _[0])\n",
    "        _data.append([league, season, match_date, home_n, away_n, url] + score + _[1])\n",
    "        _data.append([league, season, match_date, home_n, away_n, url] + score + _[2])\n",
    "    else:\n",
    "        failed_urls.append([url])\n",
    "        \n",
    "    if i%5 == 0:\n",
    "        # create DataFrame\n",
    "        df_events = pd.DataFrame(_data, columns=[\"League\", \"Season\", \"Date\", \"Home_name\", \"Away_name\", \"URL\",\n",
    "                                        \"Score_home_FT\", \"Score_away_FT\", \"Score_home_HT\", \"Score_away_HT\", \"Type\",\n",
    "                                        \"Pinn_prop\", \"Pinn_odds\", \"Odds_max\",\n",
    "                                        \"Prop_max\", \"Prop_avg\", \"Prop_median\", \"Prop_STD\"])\n",
    "        \n",
    "        # make temporal save in case webdriver crashes, loss of internet connection, ect.\n",
    "        filename = league + \"_\" + season + \"_\" + str(i)+ \".csv\"\n",
    "        \n",
    "        # save temporary file\n",
    "        df_events.to_csv(filename)\n",
    "        \n",
    "#         # turn off driver\n",
    "#         driver.quit()\n",
    "        \n",
    "#         # initiate again driver\n",
    "#         driver = webdriver.Firefox(executable_path=r\"geckodriver.exe\")\n",
    "\n",
    "#         # go to Oddsportal website\n",
    "#         driver.get(\"https://www.oddsportal.com\")\n",
    "\n",
    "#         # click on log-in button\n",
    "#         driver.find_element_by_tag_name('button').click()\n",
    "\n",
    "#         # enter User name and psw\n",
    "#         driver.find_element_by_id('login-username1').send_keys(usr)\n",
    "#         driver.find_element_by_id('login-password1').send_keys(psw, Keys.ENTER)\n",
    "\n",
    "#         # set timeout for page loadding to 30 sec\n",
    "#         driver.set_page_load_timeout(30)\n",
    "\n",
    "#         # set wait element for explicit wait\n",
    "#         wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        driver, wait = refresh_driver(driver)\n",
    "        \n",
    "    i+=1\n",
    "\n",
    "# create DataFrame\n",
    "df_events = pd.DataFrame(_data, columns=[\"League\", \"Season\", \"Date\", \"Home_name\", \"Away_name\", \"URL\",\n",
    "                                        \"Score_home_FT\", \"Score_away_FT\", \"Score_home_HT\", \"Score_away_HT\", \"Type\",\n",
    "                                        \"Pinn_prop\", \"Pinn_odds\", \"Odds_max\",\n",
    "                                        \"Prop_max\", \"Prop_avg\", \"Prop_median\", \"Prop_STD\"])\n",
    "    \n",
    "# make temporal save in case webdriver crashes, loss of internet connection, ect.\n",
    "filename = league + \"_\" + season + \"_\" + str(i)+ \".csv\"\n",
    "\n",
    "# save temporary file\n",
    "df_events.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _data\n",
    "\n",
    "df_events = pd.DataFrame(_data, columns=[\"League\", \"Season\", \"Date\", \"Home_name\", \"Away_name\", \"URL\",\n",
    "                                        \"Score_home_FT\", \"Score_away_FT\", \"Score_home_HT\", \"Score_away_HT\", \"Type\",\n",
    "                                        \"Pinn_prop\", \"Pinn_odds\", \"Odds_max\",\n",
    "                                        \"Prop_max\", \"Prop_avg\", \"Prop_median\", \"Prop_STD\"])\n",
    "\n",
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(driver.page_source)\n",
    "_ = process_1x2_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(driver.page_source)\n",
    "get_score(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_n, away_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
