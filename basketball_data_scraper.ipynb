{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "Oddsportal has dynamic content, i.e. javascript rendered page. I used `selenium` package to load and render url content with Morzilla Firefox then parsed source code to `BeautifulSoup` to extract information of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import bs4\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psw import psw, usr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for processing HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_soup(soup, league, season):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        soup: bs4.BeautifulSoup element (HTML source code parced with selenium webdriver)\n",
    "        league: string, e.g. NBA or Euro\n",
    "        season: string  e.g. 2017/1018 or 2013/2014\n",
    "    Output:\n",
    "        List of list. Each list element contains a size of 5 list\n",
    "        [total score (int), who won (1: home, 0:away), was overtime (0: Yes, 1: No), home team coef., away team coef.]\n",
    "    \"\"\"\n",
    "    _data = list()\n",
    "    rows = soup.tbody.findAll('tr')\n",
    "    for row in rows:\n",
    "        if len(row.contents) == 6:\n",
    "            # get score string\n",
    "            score = row.find(\"td\", {\"class\": \"center bold table-odds table-score\"}).text\n",
    "            # get home and away prices\n",
    "            home_price = float(row.contents[-3].text)\n",
    "            away_price = float(row.contents[-2].text)\n",
    "            # read url for detailed match coefficient analysis\n",
    "            href = row.contents[1].find('a', href=True)\n",
    "            href = \"https://www.oddsportal.com\" + href['href']\n",
    "            \n",
    "            _data.append([league, season]+process_score(score)+[home_price, away_price, href])\n",
    "    return _data\n",
    "\n",
    "\n",
    "def process_score(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        score string, e.g. \"124:114 OT\" or \"122:104\". OT (overtime)\n",
    "    Output:\n",
    "        list [total score (float), who won (1: home, 0: away), was were overtime (1: Yes, 0: No)]\n",
    "    \"\"\"\n",
    "    if len(string.split(\":\")) != 2:\n",
    "        print(\"Teams names: {}\".format(string))\n",
    "        return [np.nan, np.nan, np.nan, np.nan]\n",
    "    # check whatver there was overtime. Extract final score for home and away teams\n",
    "    if string[-2:] == \"OT\":\n",
    "        _ot = 1\n",
    "        home_score = convert_2_int(string[:-3].split(\":\")[0])\n",
    "        away_score = convert_2_int(string[:-3].split(\":\")[1])\n",
    "    else:\n",
    "        _ot = 0\n",
    "        home_score = convert_2_int(string.split(\":\")[0])\n",
    "        away_score = convert_2_int(string.split(\":\")[1])\n",
    "    \n",
    "    # check who won match\n",
    "    if home_score > away_score:\n",
    "        _win = 1\n",
    "    else:\n",
    "        _win = 0\n",
    "        \n",
    "    return [home_score, away_score, _win, _ot]\n",
    "\n",
    "\n",
    "def convert_2_int(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        string: e.g. \"123\" or \"93\"\n",
    "    Output:\n",
    "        int:\n",
    "    If ValueError, print problematic string and return NAN value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(string)\n",
    "    except ValueError:\n",
    "        print(\"Input {} could not be converted to integer\".format(string))\n",
    "        return np.nan\n",
    "    \n",
    "def convert_2_float(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        string: e.g. \"123\" or \"93\"\n",
    "    Output:\n",
    "        int:\n",
    "    If ValueError, print problematic string and return NAN value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(string)\n",
    "    except ValueError:\n",
    "        print(\"Input {} could not be converted to float\".format(string))\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate webdriver\n",
    "`executable_path` points to excecutable used to connect to Firefox. To use different browser download approprate geckodriver (hyperlink) or refer to this Stack exchange post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "driver = webdriver.Firefox(executable_path=r\"geckodriver.exe\")\n",
    "\n",
    "# go to Oddsportal website\n",
    "driver.get(\"https://www.oddsportal.com\")\n",
    "\n",
    "# click on log-in button\n",
    "driver.find_element_by_tag_name('button').click()\n",
    "\n",
    "# enter User name and psw\n",
    "driver.find_element_by_id('login-username1').send_keys(usr)\n",
    "driver.find_element_by_id('login-password1').send_keys(psw, Keys.ENTER)\n",
    "\n",
    "# set timeout for page loadding to 30 sec\n",
    "driver.set_page_load_timeout(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NBA and Euroleague 2013-2018 season data\n",
    "* `season_dict` dictonary keys are  league and season names, while values are tuples (url, number of pages to iterate over).\n",
    "* itrate over season url pages and read team scored points and home/away team average coefficients\n",
    "* transform data into pandas DataFrame and save it as .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "season_dict = {\n",
    "    \"NBA_2017/2018\": (\"https://www.oddsportal.com/basketball/usa/nba-2017-2018/results/#/page/\", 28),\n",
    "    \"NBA_2016/2017\": (\"https://www.oddsportal.com/basketball/usa/nba-2016-2017/results/#/page/\", 29),\n",
    "    \"NBA_2015/2016\": (\"https://www.oddsportal.com/basketball/usa/nba-2015-2016/results/#/page/\", 29),\n",
    "    \"NBA_2014/2015\": (\"https://www.oddsportal.com/basketball/usa/nba-2014-2015/results/#/page/\", 29),\n",
    "    \"NBA_2013/2014\": (\"https://www.oddsportal.com/basketball/usa/nba-2013-2014/results/#/page/\", 29),\n",
    "    \"EURO_2017/2018\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2017-2018/results/#/page/\", 6),\n",
    "    \"EURO_2016/2017\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2016-2017/results/#/page/\", 6),\n",
    "    \"EURO_2015/2016\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2015-2016/results/#/page/\", 5),\n",
    "    \"EURO_2014/2015\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2014-2015/results/#/page/\", 6),\n",
    "    \"EURO_2013/2014\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2013-2014/results/#/page/\", 6)\n",
    "}\n",
    "\n",
    "all_data = list()\n",
    "for key in season_dict.keys():\n",
    "    # Load main url page\n",
    "    url = season_dict[key][0]\n",
    "    # Extract league and season from key string\n",
    "    league = key.split(\"_\")[0]\n",
    "    season = key.split(\"_\")[1]\n",
    "    # Iterate over all pages for particular season\n",
    "    for idx in range(1, season_dict[key][1]+1):\n",
    "        # Load page\n",
    "        driver.get(url+str(idx))\n",
    "        # quick and dirty fix, implicit wait for 1.5 sec,  so that page is really loaded\n",
    "        time.sleep(5)\n",
    "        # Process HTLM into data\n",
    "        soup = bs4.BeautifulSoup(driver.page_source)\n",
    "        all_data += process_soup(soup, league, season)\n",
    "        \n",
    "df_1 = pd.DataFrame(all_data, columns=[\"League\", \"Season\", \"Home_score\", \"Away_score\",\n",
    "                                       \"Win\", \"OT\", \"Home_p\", \"Away_p\", \"URL\"])\n",
    "df_1.to_csv(\"basketball_scores.csv\")\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_1 = pd.read_csv(\"basketball_scores.csv\", index_col=0)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "df_1.groupby([\"League\", \"Season\"])[\"URL\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze matches in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ah_ou_coef(soup):\n",
    "    \"\"\"\n",
    "    Returns asian handicap coefficient or under/over totals, which were offered by largest number of book makers\n",
    "    Same logic applies for both type pages\n",
    "    input:\n",
    "        market_type: boolean, tells how to read asian handicap or over/under totals\n",
    "        soup: bs4.BeautifulSoup element (HTML source code parced with selenium webdriver)\n",
    "    output:\n",
    "        list: [asian handicap, home coef., away coef.]\n",
    "        or\n",
    "        list: [asian handicap, home coef., away coef.]\n",
    "    \"\"\"\n",
    "    table = soup.find(\"div\", {\"id\": \"odds-data-table\"})\n",
    "    table = table.findAll(\"div\", {\"class\": \"table-container\"})\n",
    "    max_book_count = 0\n",
    "    max_ah = np.nan\n",
    "    for row in table:\n",
    "        if row.text != \"BETTING EXCHANGES\":\n",
    "            odd_count = int(row.find(\"span\", {\"class\":\"odds-cnt\"}).text[1:-1])\n",
    "            if odd_count > max_book_count:\n",
    "                max_book_count = odd_count\n",
    "                bet_type = row.strong.text\n",
    "                price_1 = convert_2_float(row.findAll('span')[1].text)\n",
    "                price_2 = convert_2_float(row.findAll('span')[2].text)\n",
    "    return [bet_type, price_1, price_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_league = \"NBA\"\n",
    "_season = \"2016/2017\"\n",
    "filename = _league + \"_\" + _season.replace(\"/\",\"_\") + \".csv\"\n",
    "cond_1 = df_1.League == _league\n",
    "cond_2 = df_1.Season == _season\n",
    "df_c = df_1[cond_1 & cond_2]\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read NBA or Euro League detailed stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "all_data = list()\n",
    "i = 0\n",
    "for url in missing_df.URL.values:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(\"page not loaded\")\n",
    "    ah_prices = [np.nan, np.nan, np.nan]\n",
    "    # get soup\n",
    "    soup = bs4.BeautifulSoup(driver.page_source)\n",
    "    # get team names\n",
    "    try:\n",
    "        names = soup.h1.text.split(\" - \")\n",
    "        name_h = names[0]\n",
    "        name_a = names[1]\n",
    "    except Exception as e:\n",
    "        name_h = np.nan\n",
    "        name_a = np.nan\n",
    "        print(e)\n",
    "        print(\"Team names not found\")\n",
    "    # get match date\n",
    "    try:\n",
    "        match_date = soup.find(\"div\", {\"id\": \"col-content\"}).p.text\n",
    "    except:\n",
    "        match_date = np.nan    \n",
    "    try:\n",
    "        # click AH button\n",
    "        driver.find_element_by_xpath(\"//span[contains(text(), 'AH')]\").click()\n",
    "        wait.until(EC.element_to_be_clickable((By.ID, 'odds-data-table')))\n",
    "        # get soup\n",
    "        soup = bs4.BeautifulSoup(driver.page_source)\n",
    "        ah_prices = get_ah_ou_coef(soup)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"AH prices not read\")  \n",
    "    ou_prices = [np.nan, np.nan, np.nan]\n",
    "    try:\n",
    "        # click OU button\n",
    "        driver.find_element_by_xpath(\"//span[contains(text(), 'O/U')]\").click()\n",
    "        wait.until(EC.element_to_be_clickable((By.ID, 'odds-data-table')))\n",
    "        # get soup\n",
    "        soup = bs4.BeautifulSoup(driver.page_source)\n",
    "        ou_prices = get_ah_ou_coef(soup)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"OU prices not read\")\n",
    "    # add new data\n",
    "    all_data.append(ah_prices + ou_prices + [name_h, name_a, match_date, url])\n",
    "    # save temporary data:\n",
    "    if i%100==0:\n",
    "        # make temporal save in case webdriver crashes, loss of internet connection, ect.\n",
    "        filename = _league + \"_\" + _season.replace(\"/\",\"_\") + \"_\" + str(i-100) + \"_\" + str(i)+ \".csv\"\n",
    "        df_2 = pd.DataFrame(all_data, columns=[\"AH\", \"AH_Home\", \"AH_Away\",\n",
    "                                       \"OU\", \"Over\", \"Under\",\n",
    "                                       \"Home_name\", \"Away_name\", \"Date\", \"URL\"])\n",
    "        df_2.to_csv(filename)\n",
    "    i+=1\n",
    "filename = _league + \"_\" + _season.replace(\"/\",\"_\") + \"_\" + str(i)+ \".csv\"\n",
    "df_2 = pd.DataFrame(all_data, columns=[\"AH\", \"AH_Home\", \"AH_Away\",\n",
    "                                       \"OU\", \"Over\", \"Under\",\n",
    "                                       \"Home_name\", \"Away_name\", \"Date\", \"URL\"])\n",
    "df_2.to_csv(filename)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn off webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
