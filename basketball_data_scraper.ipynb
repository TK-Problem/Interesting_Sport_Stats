{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "Oddsportal has dynamic content, i.e. javascript rendered page. I used `selenium` package to load and render url content with Morzilla Firefox then parsed source code to `BeautifulSoup` to extract information of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import bs4\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from psw import psw, usr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for processing HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_href(soup, league, season):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        soup: bs4.BeautifulSoup element (HTML source code parced with selenium webdriver)\n",
    "        league: string, e.g. NBA or Euro\n",
    "        season: string  e.g. 2017/1018 or 2013/2014\n",
    "    Output:\n",
    "        List of list. Each list element contains a size of 3\n",
    "        [league, season, url (unique for each game)]\n",
    "    \"\"\"\n",
    "    _data = list()\n",
    "    rows = soup.tbody.findAll('tr')\n",
    "    for row in rows:\n",
    "        if len(row.contents) == 6:\n",
    "            # read url for detailed match coefficient analysis\n",
    "            href = row.contents[1].find('a', href=True)\n",
    "            href = \"https://www.oddsportal.com\" + href['href']\n",
    "            \n",
    "            _data.append([league, season, href])\n",
    "    return _data\n",
    "\n",
    "def convert_2_int(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        string: e.g. \"123\" or \"93\"\n",
    "    Output:\n",
    "        int:\n",
    "    If ValueError, print problematic string and return NAN value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(string)\n",
    "    except ValueError:\n",
    "        print(\"Input {} could not be converted to integer\".format(string))\n",
    "        return np.nan\n",
    "    \n",
    "def convert_2_float(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        string: e.g. \"123\" or \"93\"\n",
    "    Output:\n",
    "        int:\n",
    "    If ValueError, print problematic string and return NAN value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return float(string)\n",
    "    except ValueError:\n",
    "        print(\"Input {} could not be converted to float\".format(string))\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate webdriver\n",
    "`executable_path` points to excecutable used to connect to Firefox. To use different browser download approprate geckodriver (hyperlink) or refer to this Stack exchange post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "driver = webdriver.Firefox(executable_path=r\"geckodriver.exe\")\n",
    "\n",
    "# go to Oddsportal website\n",
    "driver.get(\"https://www.oddsportal.com\")\n",
    "\n",
    "# click on log-in button\n",
    "driver.find_element_by_tag_name('button').click()\n",
    "\n",
    "# enter User name and psw\n",
    "driver.find_element_by_id('login-username1').send_keys(usr)\n",
    "driver.find_element_by_id('login-password1').send_keys(psw, Keys.ENTER)\n",
    "\n",
    "# set timeout for page loadding to 30 sec\n",
    "driver.set_page_load_timeout(30)\n",
    "\n",
    "# set wait element for explicit wait\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spesific basketball league season data\n",
    "* `season_dict` dictonary keys are  league and season names, while values are tuples (url, number of pages to iterate over).\n",
    "* itrate over season url pages and read team scored points and home/away team average coefficients\n",
    "* transform data into pandas DataFrame and save it as .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league = \"Euroleague\"\n",
    "season = \"2015_2016\"\n",
    "season_url = \"https://www.oddsportal.com/basketball/europe/euroleague-2015-2016/results/#/page/\"\n",
    "# number of pages for the season\n",
    "no_pages = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# temporary list to store data\n",
    "_data = list()\n",
    "\n",
    "for idx in range(1, no_pages+1):\n",
    "    # Load page\n",
    "    driver.get(season_url+str(idx))\n",
    "    # explicitly wait untill page is loaded\n",
    "    wait.until(EC.visibility_of_element_located((By.ID, 'tournamentTable')))\n",
    "    # Process HTLM into data\n",
    "    soup = bs4.BeautifulSoup(driver.page_source)\n",
    "    _data += get_unique_href(soup, league, season)\n",
    "\n",
    "# create dataframe with unqiue URLS\n",
    "df_urls = pd.DataFrame(_data, columns=[\"League\", \"Season\", \"URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze matches in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_names(soup):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        soup: BS4 soup element\n",
    "    \"\"\"\n",
    "    try:\n",
    "        names = soup.h1.text.split(\" - \")\n",
    "        return names[0], names[1]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Team names not found\")\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "            \n",
    "def load_page(driver, url):\n",
    "    \"\"\"\n",
    "    Return true if page was loaded correctly, else if error occured\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"page not loaded\")\n",
    "        print(f\"{e}\")\n",
    "        return False\n",
    "    \n",
    "def get_match_date(soup):\n",
    "    \"\"\"\n",
    "    Return date as string\n",
    "    \"\"\"   \n",
    "    try:\n",
    "        match_date = soup.find(\"div\", {\"id\": \"col-left\"}).p.text\n",
    "        datetime_object = datetime.strptime(match_date, '%A, %d %b %Y, %H:%M')\n",
    "        return datetime_object\n",
    "    except:\n",
    "        return np.nan  \n",
    "    \n",
    "def get_opening_odds(driver):\n",
    "    \"\"\"\n",
    "    Return closing odds\n",
    "    \"\"\"\n",
    "    soup = bs4.BeautifulSoup(driver.page_source)\n",
    "    try:\n",
    "        _coef_open = soup.find(\"span\", {\"id\": \"tooltiptext\"}).contents[-2].text\n",
    "        return convert_2_float(_coef_open)\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def get_score(soup):\n",
    "    \"\"\"\n",
    "    Return\n",
    "    \"\"\"\n",
    "    element = soup.find(\"p\", {\"class\": \"result\"})\n",
    "    quater_scores = element.contents[-1].split(\",\")\n",
    "    if len(element.strong.text.split(\"OT\"))==1:\n",
    "        OT = False\n",
    "        home_s = convert_2_int(element.strong.text.split(\":\")[0])\n",
    "        away_s = convert_2_int(element.strong.text.split(\":\")[1])\n",
    "    else:\n",
    "        OT = True\n",
    "        score_string = element.strong.text.split(\"OT\")[0][:-1]\n",
    "        home_s = convert_2_int(score_string.split(\":\")[0])\n",
    "        away_s = convert_2_int(score_string.split(\":\")[1])\n",
    "        \n",
    "    # get quater scores    \n",
    "    Q1 = quater_scores[0][2:].split(\":\")\n",
    "    Q1_h = convert_2_int(Q1[0])\n",
    "    Q1_a = convert_2_int(Q1[1])\n",
    "    Q2 = quater_scores[1][1:].split(\":\")\n",
    "    Q2_h = convert_2_int(Q2[0])\n",
    "    Q2_a = convert_2_int(Q2[1])\n",
    "    Q3 = quater_scores[2][1:].split(\":\")\n",
    "    Q3_h = convert_2_int(Q3[0])\n",
    "    Q3_a = convert_2_int(Q3[1])\n",
    "    Q4 = quater_scores[3][1:-1].split(\":\")\n",
    "    Q4_h = convert_2_int(Q4[0])\n",
    "    Q4_a = convert_2_int(Q4[1])\n",
    "        \n",
    "    return [home_s, away_s, OT, Q1_h, Q1_a, Q2_h, Q2_a, Q3_h, Q3_a, Q4_h, Q4_a]\n",
    "    \n",
    "def get_h2h_coef(driver):\n",
    "    \"\"\"\n",
    "    Return list of list with \n",
    "    \"\"\"\n",
    "    main_table = driver.find_element_by_xpath(\"//table[@class='table-main detail-odds sortable']\")\n",
    "    main_table = main_table.find_element_by_tag_name('tbody')\n",
    "    _data = [np.nan]*12\n",
    "    for _ in main_table.find_elements_by_tag_name('tr'):\n",
    "        # filter out empty rows\n",
    "        row = _.find_elements_by_tag_name('td')\n",
    "        if len(row) == 5:\n",
    "            book_name = row[0].text[1:-2]\n",
    "            # exclude empty rows\n",
    "            if book_name == '':\n",
    "                continue\n",
    "            # remove new bookmaker tags\n",
    "            if book_name[-4:] == '\\nNEW':\n",
    "                book_name = book_name.replace('\\nNEW', '')\n",
    "\n",
    "            if book_name in [\"bet365\", \"Dafabet\", \"Pinnacle\"]:\n",
    "                # read different bookmaker prices\n",
    "\n",
    "                # H2H home coef.\n",
    "                _element = row[1]\n",
    "                _coef_close = convert_2_float(_element.text)\n",
    "                ActionChains(driver).move_to_element(_element).perform()\n",
    "                _coef_open = get_opening_odds(driver)\n",
    "                if book_name==\"bet365\":\n",
    "                    _data[0] = _coef_close\n",
    "                    _data[1] = _coef_open\n",
    "                elif book_name==\"Dafabet\":\n",
    "                    _data[4] = _coef_close\n",
    "                    _data[5] = _coef_open\n",
    "                else:\n",
    "                    _data[8] = _coef_close\n",
    "                    _data[9] = _coef_open\n",
    "\n",
    "                # H2H away coef.\n",
    "                _element = row[2]\n",
    "                _coef_close = convert_2_float(_element.text)\n",
    "                ActionChains(driver).move_to_element(_element).perform()\n",
    "                _coef_open = get_opening_odds(driver)\n",
    "                if book_name==\"bet365\":\n",
    "                    _data[2] = _coef_close\n",
    "                    _data[3] = _coef_open\n",
    "                elif book_name==\"Dafabet\":\n",
    "                    _data[6] = _coef_close\n",
    "                    _data[7] = _coef_open\n",
    "                else:\n",
    "                    _data[10] = _coef_close\n",
    "                    _data[11] = _coef_open\n",
    "    \n",
    "    return _data        \n",
    "\n",
    "def click_AH_OU_button(driver, wait, AH=True):\n",
    "    try:\n",
    "        if AH:\n",
    "            # click AH button\n",
    "            driver.find_element_by_xpath(\"//span[contains(text(), 'AH')]\").click()\n",
    "        else:\n",
    "            # click OU button\n",
    "            driver.find_element_by_xpath(\"//span[contains(text(), 'O/U')]\").click()\n",
    "        wait.until(EC.element_to_be_clickable((By.ID, 'odds-data-table')))\n",
    "    except Exception as e:\n",
    "        print(\"Button not found\")\n",
    "        print(e)\n",
    "        \n",
    "def click_xpath(driver, xpath):\n",
    "    \"\"\"\n",
    "    Tries to click on Xpath element\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath).click()\n",
    "    except Exception as e:\n",
    "        print(\"Error {} occured\".format(e))\n",
    "        \n",
    "def click_max_book_market(driver, soup, ah = True):\n",
    "    \"\"\"\n",
    "    Find and click on AH or OU market with highest number of bookmakers\n",
    "    \"\"\"\n",
    "    soup = bs4.BeautifulSoup(driver.page_source)\n",
    "    rows = soup.find(\"div\", {\"id\": \"odds-data-table\"}).findAll(\"div\", {\"class\": \"table-container\"})\n",
    "    \n",
    "    max_book_count = 0\n",
    "    max_type = np.nan\n",
    "    for row in rows:\n",
    "        if row.strong is not None:\n",
    "            if row.strong.text[:5] != \"Click\":\n",
    "                if convert_2_int(row.find(\"span\", {\"class\": \"odds-cnt\"}).text[1:-1]) > max_book_count:\n",
    "                    max_book_count = convert_2_int(row.find(\"span\", {\"class\": \"odds-cnt\"}).text[1:-1])\n",
    "                    if ah:\n",
    "                        max_type = row.strong.text[15:]\n",
    "                    else:\n",
    "                        max_type = row.strong.text[11:]\n",
    "\n",
    "    if ah:\n",
    "        _xpath = \"//*[contains(text(), 'Asian handicap \" + max_type + \"')]\"\n",
    "    else:\n",
    "        _xpath = \"//*[contains(text(), 'Over/Under \" + max_type + \"')]\"\n",
    "\n",
    "    click_xpath(driver, _xpath)\n",
    "    \n",
    "\n",
    "def get_ou_prices(soup):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        soup: bs4 soup element\n",
    "    Output:\n",
    "        list of lists\n",
    "    \"\"\"\n",
    "    rows = soup.find(\"div\", {\"id\": \"odds-data-table\"}).findAll(\"div\", {\"class\": \"table-container\"})\n",
    "    _data_list = _data_list = [np.nan]*7\n",
    "    for row in rows:\n",
    "        avg_p = row.find(\"tr\", {\"class\": \"aver\"})\n",
    "        max_p = row.find(\"tr\", {\"class\": \"highest\"})\n",
    "        if avg_p is not None and max_p is not None and row.find(\"a\", string=\"Pinnacle\") is not None:\n",
    "            bet_type = convert_2_float(row.contents[0].strong.text[12:-1])\n",
    "            _prices = row.find(\"a\", string=\"Pinnacle\").parent.parent.parent.findAll(\"td\")\n",
    "            _o = convert_2_float(_prices[2].text)\n",
    "            _u = convert_2_float(_prices[3].text)\n",
    "            _data_list[0] = bet_type\n",
    "            _data_list[1] = _o\n",
    "            _data_list[2] = _u\n",
    "            # get average prices\n",
    "            _data_list[3] = convert_2_float(avg_p.findAll('td')[2].text)\n",
    "            _data_list[4] = convert_2_float(avg_p.findAll('td')[3].text)\n",
    "            # get max available prices\n",
    "            _data_list[5] = convert_2_float(max_p.findAll('td')[2].text)\n",
    "            _data_list[6] = convert_2_float(max_p.findAll('td')[3].text) \n",
    "            \n",
    "    return _data_list\n",
    "\n",
    "\n",
    "def get_ah_prices(soup):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        soup: bs4 soup element\n",
    "    Output:\n",
    "        list of lists\n",
    "    \"\"\"\n",
    "    rows = soup.find(\"div\", {\"id\": \"odds-data-table\"}).findAll(\"div\", {\"class\": \"table-container\"})\n",
    "    _data_list = _data_list = [np.nan]*7\n",
    "    for row in rows:\n",
    "        avg_p = row.find(\"tr\", {\"class\": \"aver\"})\n",
    "        max_p = row.find(\"tr\", {\"class\": \"highest\"})\n",
    "        if avg_p is not None and max_p is not None and row.find(\"a\", string=\"Pinnacle\") is not None:\n",
    "            bet_type = convert_2_float(row.contents[0].strong.text[15:-1])\n",
    "            _prices = row.find(\"a\", string=\"Pinnacle\").parent.parent.parent.findAll(\"td\")\n",
    "            _o = convert_2_float(_prices[2].text)\n",
    "            _u = convert_2_float(_prices[3].text)\n",
    "            _o_prop = round(((_u + _o) / _u), 2) \n",
    "            _u_prop = round(((_u + _o) / _o), 2)\n",
    "            _data_list[0] = bet_type\n",
    "            _data_list[1] = _o\n",
    "            _data_list[2] = _u\n",
    "            # get average prices\n",
    "            _data_list[3] = convert_2_float(avg_p.findAll('td')[2].text)\n",
    "            _data_list[4] = convert_2_float(avg_p.findAll('td')[3].text)\n",
    "            # get max available prices\n",
    "            _data_list[5] = convert_2_float(max_p.findAll('td')[2].text)\n",
    "            _data_list[6] = convert_2_float(max_p.findAll('td')[3].text) \n",
    "            \n",
    "    return _data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read detailed basketball match stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# temporary list to store data\n",
    "_data = list()\n",
    "failed_urls = list()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for url in df_urls.URL.values:\n",
    "    if load_page(driver, url):\n",
    "        # wait till page is loaded\n",
    "        wait.until(EC.visibility_of_element_located((By.ID , \"odds-data-table\")))\n",
    "        # scroll down, so that all bookmakers are accecible\n",
    "        driver.execute_script(\"window.scrollTo(0, 320)\")\n",
    "        # get soup element\n",
    "        soup = bs4.BeautifulSoup(driver.page_source)\n",
    "        # read team names\n",
    "        home_n, away_n = get_team_names(soup)\n",
    "        # get match date\n",
    "        match_date = get_match_date(soup)\n",
    "        # get score\n",
    "        score = get_score(soup)\n",
    "        # get H2H bookmaker prices\n",
    "        _coefs_H2H = get_h2h_coef(driver)\n",
    "        \n",
    "        # get Asian handicap info\n",
    "        try:\n",
    "            # click AH button\n",
    "            click_AH_OU_button(driver, wait)\n",
    "            # click on max book AH market\n",
    "            click_max_book_market(driver, bs4.BeautifulSoup(driver.page_source))\n",
    "            # get AH bookmaker prices\n",
    "            _coefs_AH = get_ah_prices(bs4.BeautifulSoup(driver.page_source))\n",
    "        except:\n",
    "            _coefs_AH = [np.nan]*7\n",
    "        \n",
    "        # get OU info\n",
    "        try:\n",
    "            # click OU button\n",
    "            click_AH_OU_button(driver, wait, False)\n",
    "            # click on max book OU market\n",
    "            click_max_book_market(driver, bs4.BeautifulSoup(driver.page_source), False)\n",
    "            # get OU bookmaker prices\n",
    "            _coefs_OU = get_ou_prices(bs4.BeautifulSoup(driver.page_source))\n",
    "        except:\n",
    "            _coefs_OU = [np.nan]*7\n",
    "        \n",
    "        \n",
    "        # add all data\n",
    "        _data.append([match_date, home_n, away_n, url] + score + _coefs_H2H + _coefs_AH + _coefs_OU)\n",
    "    else:\n",
    "        failed_urls.append([url])\n",
    "        \n",
    "    \n",
    "    \n",
    "    if i%50 == 0:\n",
    "        # create DataFrame\n",
    "        df_events = pd.DataFrame(_data, columns=[\"Date\", \"Home_n\", \"Away_n\", \"URL\", \"Home_score\", \"Away_score\", \"OT\",\n",
    "                                                \"Q1_home\", \"Q1_away\", \"Q2_home\", \"Q2_away\", \"Q3_Home\", \"Q3_away\",\n",
    "                                                \"Q4_home\", \"Q4_away\",\n",
    "                                                \"H2H_home_bet365_close\", \"H2H_home_bet365_open\",\n",
    "                                                \"H2H_away_bet365_close\", \"H2H_away_bet365_open\",\n",
    "                                                \"H2H_home_Dafa_close\", \"H2H_home_Dafa_open\",\n",
    "                                                \"H2H_away_Dafa_close\", \"H2H_away_Dafa_open\",\n",
    "                                                \"H2H_home_Pinnacle_close\", \"H2H_home_Pinnacle_open\",\n",
    "                                                \"H2H_away_Pinnacle_close\", \"H2H_away_Pinnacle_open\",\n",
    "                                                \"AH_line\", \"Pinn_home_AH_close\",\n",
    "                                                \"Pinn_away_AH_close\", \"Avg_home_AH_price\",\n",
    "                                                \"Avg_away_AH_price\", \"Max_home_AH_price\", \"Max_away_AH_price\",\n",
    "                                                \"OU_line\", \"Pinn_over_price_close\",\n",
    "                                                \"Pinn_under_price_close\", \"Avg_over_price\",\n",
    "                                                \"Avg_under_price\", \"Max_over_price\", \"Max_under_price\"])\n",
    "        # make temporal save in case webdriver crashes, loss of internet connection, ect.\n",
    "        filename = league + \"_\" + season + \"_\" + str(i)+ \".csv\"\n",
    "        \n",
    "        # save temporary file\n",
    "        df_events.to_csv(filename)\n",
    "        \n",
    "    i+=1\n",
    "    \n",
    "# create DataFrame\n",
    "df_events = pd.DataFrame(_data, columns=[\"Date\", \"Home_n\", \"Away_n\", \"URL\", \"Home_score\", \"Away_score\", \"OT\",\n",
    "                                        \"Q1_home\", \"Q1_away\", \"Q2_home\", \"Q2_away\", \"Q3_Home\", \"Q3_away\",\n",
    "                                        \"Q4_home\", \"Q4_away\",\n",
    "                                        \"H2H_home_bet365_close\", \"H2H_home_bet365_open\",\n",
    "                                        \"H2H_away_bet365_close\", \"H2H_away_bet365_open\",\n",
    "                                        \"H2H_home_Dafa_close\", \"H2H_home_Dafa_open\",\n",
    "                                        \"H2H_away_Dafa_close\", \"H2H_away_Dafa_open\",\n",
    "                                        \"H2H_home_Pinnacle_close\", \"H2H_home_Pinnacle_open\",\n",
    "                                        \"H2H_away_Pinnacle_close\", \"H2H_away_Pinnacle_open\",\n",
    "                                        \"AH_line\", \"Pinn_home_AH_close\",\n",
    "                                        \"Pinn_away_AH_close\", \"Avg_home_AH_price\",\n",
    "                                        \"Avg_away_AH_price\", \"Max_home_AH_price\", \"Max_away_AH_price\",\n",
    "                                        \"OU_line\", \"Pinn_over_price_close\",\n",
    "                                        \"Pinn_under_price_close\", \"Avg_over_price\",\n",
    "                                        \"Avg_under_price\", \"Max_over_price\", \"Max_under_price\"])\n",
    "# make temporal save in case webdriver crashes, loss of internet connection, ect.\n",
    "filename = league + \"_\" + season + \"_\" + str(i)+ \".csv\"\n",
    "\n",
    "# save temporary file\n",
    "df_events.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"Euroleague_2015_2016_250.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls = df_[(df_.H2H_away_Pinnacle_open.isnull()) | (df_.Pinn_away_AH_close.isnull())]\n",
    "# df_urls\n",
    "df_.isnull().sum()\n",
    "df_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_league = \"NBA\"\n",
    "_season = \"2018/2019\"\n",
    "filename = _league + \"_\" + _season.replace(\"/\",\"_\") + \".csv\"\n",
    "cond_1 = df_1.League == _league\n",
    "cond_2 = df_1.Season == _season\n",
    "df_c = df_1[cond_1 & cond_2]\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read NBA or Euro League detailed stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wait = WebDriverWait(driver, 20)\n",
    "all_data = list()\n",
    "i = 0\n",
    "for url in df_c.URL.values:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(\"page not loaded\")\n",
    "    ah_prices = [np.nan, np.nan, np.nan]\n",
    "    # get soup\n",
    "    soup = bs4.BeautifulSoup(driver.page_source)\n",
    "    # get team names\n",
    "    try:\n",
    "        names = soup.h1.text.split(\" - \")\n",
    "        name_h = names[0]\n",
    "        name_a = names[1]\n",
    "    except Exception as e:\n",
    "        name_h = np.nan\n",
    "        name_a = np.nan\n",
    "        print(e)\n",
    "        print(\"Team names not found\")\n",
    "    # get match date\n",
    "    try:\n",
    "        match_date = soup.find(\"div\", {\"id\": \"col-content\"}).p.text\n",
    "    except:\n",
    "        match_date = np.nan    \n",
    "    try:\n",
    "        # click AH button\n",
    "        driver.find_element_by_xpath(\"//span[contains(text(), 'AH')]\").click()\n",
    "        wait.until(EC.element_to_be_clickable((By.ID, 'odds-data-table')))\n",
    "        # get soup\n",
    "        soup = bs4.BeautifulSoup(driver.page_source)\n",
    "        ah_prices = get_ah_ou_coef(soup)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"AH prices not read\")  \n",
    "    ou_prices = [np.nan, np.nan, np.nan]\n",
    "    try:\n",
    "        # click OU button\n",
    "        driver.find_element_by_xpath(\"//span[contains(text(), 'O/U')]\").click()\n",
    "        wait.until(EC.element_to_be_clickable((By.ID, 'odds-data-table')))\n",
    "        # get soup\n",
    "        soup = bs4.BeautifulSoup(driver.page_source)\n",
    "        ou_prices = get_ah_ou_coef(soup)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"OU prices not read\")\n",
    "    # add new data\n",
    "    all_data.append(ah_prices + ou_prices + [name_h, name_a, match_date, url])\n",
    "    # save temporary data:\n",
    "    if i%100==0:\n",
    "        # make temporal save in case webdriver crashes, loss of internet connection, ect.\n",
    "        filename = _league + \"_\" + _season.replace(\"/\",\"_\") + \"_\" + str(i-100) + \"_\" + str(i)+ \".csv\"\n",
    "        df_2 = pd.DataFrame(all_data, columns=[\"AH\", \"AH_Home\", \"AH_Away\",\n",
    "                                       \"OU\", \"Over\", \"Under\",\n",
    "                                       \"Home_name\", \"Away_name\", \"Date\", \"URL\"])\n",
    "        df_2.to_csv(filename)\n",
    "    i+=1\n",
    "filename = _league + \"_\" + _season.replace(\"/\",\"_\") + \"_\" + str(i)+ \".csv\"\n",
    "df_2 = pd.DataFrame(all_data, columns=[\"AH\", \"AH_Home\", \"AH_Away\",\n",
    "                                       \"OU\", \"Over\", \"Under\",\n",
    "                                       \"Home_name\", \"Away_name\", \"Date\", \"URL\"])\n",
    "df_2.to_csv(filename)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn off webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "season_dict = {\n",
    "    \"NBA_2018/2019\": (\"https://www.oddsportal.com/basketball/usa/nba/results//#/page/\", 8)\n",
    "    \"NBA_2017/2018\": (\"https://www.oddsportal.com/basketball/usa/nba-2017-2018/results/#/page/\", 28),\n",
    "    \"NBA_2016/2017\": (\"https://www.oddsportal.com/basketball/usa/nba-2016-2017/results/#/page/\", 29),\n",
    "    \"NBA_2015/2016\": (\"https://www.oddsportal.com/basketball/usa/nba-2015-2016/results/#/page/\", 29),\n",
    "    \"NBA_2014/2015\": (\"https://www.oddsportal.com/basketball/usa/nba-2014-2015/results/#/page/\", 29),\n",
    "    \"NBA_2013/2014\": (\"https://www.oddsportal.com/basketball/usa/nba-2013-2014/results/#/page/\", 29),\n",
    "    \"EURO_2017/2018\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2017-2018/results/#/page/\", 6),\n",
    "    \"EURO_2016/2017\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2016-2017/results/#/page/\", 6),\n",
    "    \"EURO_2015/2016\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2015-2016/results/#/page/\", 5),\n",
    "    \"EURO_2014/2015\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2014-2015/results/#/page/\", 6),\n",
    "    \"EURO_2013/2014\": (\"https://www.oddsportal.com/basketball/europe/euroleague-2013-2014/results/#/page/\", 6)\n",
    "}\n",
    "\n",
    "all_data = list()\n",
    "for key in season_dict.keys():\n",
    "    # Load main url page\n",
    "    url = season_dict[key][0]\n",
    "    # Extract league and season from key string\n",
    "    league = key.split(\"_\")[0]\n",
    "    season = key.split(\"_\")[1]\n",
    "    # Iterate over all pages for particular season\n",
    "    for idx in range(1, season_dict[key][1]+1):\n",
    "        # Load page\n",
    "        driver.get(url+str(idx))\n",
    "        # quick and dirty fix, implicit wait for 1.5 sec,  so that page is really loaded\n",
    "        time.sleep(5)\n",
    "        # Process HTLM into data\n",
    "        soup = bs4.BeautifulSoup(driver.page_source)\n",
    "        all_data += process_soup(soup, league, season)\n",
    "        \n",
    "df_1 = pd.DataFrame(all_data, columns=[\"League\", \"Season\", \"Home_score\", \"Away_score\",\n",
    "                                       \"Win\", \"OT\", \"Home_p\", \"Away_p\", \"URL\"])\n",
    "df_1.to_csv(\"basketball_scores.csv\")\n",
    "df_1.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
